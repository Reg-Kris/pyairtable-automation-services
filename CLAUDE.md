# Automation Services - Claude Context

## üéØ Service Purpose
This is the **automation and file processing engine** of the PyAirtable ecosystem - a consolidated microservice that combines file processing and workflow automation capabilities. It handles multi-format file processing, content extraction, and complex workflow automation with scheduling and triggers.

## üèóÔ∏è Current State

### Deployment Status
- **Environment**: ‚úÖ Local Kubernetes (Minikube)
- **Services Running**: ‚úÖ 7 out of 9 services operational
- **Database Analysis**: ‚úÖ Airtable test database analyzed (34 tables, 539 fields)
- **Metadata Tool**: ‚úÖ Table analysis tool executed successfully

### Service Status
- **File Processing**: ‚úÖ Multi-format support (PDF, DOC, DOCX, TXT, CSV, XLSX, XLS)
- **Content Extraction**: ‚úÖ Intelligent text extraction with metadata
- **Workflow Engine**: ‚úÖ CRUD operations with cron-based scheduling
- **File Management**: ‚úÖ Upload, process, retrieve, and delete operations
- **Background Processing**: ‚úÖ Unified task management
- **Cross-Service Integration**: ‚úÖ Files can trigger workflows automatically
- **Database**: ‚úÖ PostgreSQL with SQLAlchemy ORM
- **Caching**: ‚úÖ Redis for background task management

### Recent Fixes Applied
- ‚úÖ Pydantic v2 compatibility issues resolved
- ‚úÖ Gemini ThinkingConfig configuration fixed
- ‚úÖ SQLAlchemy metadata handling updated
- ‚úÖ Service deployment to Kubernetes completed

## üîß Technical Details
- **Framework**: FastAPI with async/await
- **Database**: PostgreSQL with SQLAlchemy ORM
- **Cache**: Redis for background tasks
- **File Processing**: Multi-format content extraction
- **Scheduling**: Cron-based with APScheduler
- **Python**: 3.11+
- **Port**: 8006

## üìã API Endpoints

### File Processing Routes (/files/*)
```python
POST   /files/upload                    # Upload file for processing
GET    /files/{file_id}                # Get file info and status
GET    /files                          # List files with filtering
POST   /files/process/{file_id}        # Process uploaded file
GET    /files/extract/{file_id}        # Extract content from file
DELETE /files/{file_id}                # Delete file and data
```

### Workflow Management Routes (/workflows/*)
```python
POST   /workflows                      # Create new workflow
GET    /workflows                      # List workflows with filtering
GET    /workflows/{workflow_id}        # Get workflow details
PUT    /workflows/{workflow_id}        # Update workflow configuration
DELETE /workflows/{workflow_id}        # Delete workflow
POST   /workflows/{workflow_id}/trigger # Manually trigger workflow
GET    /workflows/executions           # List workflow executions
GET    /workflows/executions/{exec_id} # Get execution details
```

### System Routes
```python
GET    /health                         # Health check with component status
```

## üõ†Ô∏è File Processing Features

### Supported File Types
- **PDF**: Text extraction with page metadata
- **DOC/DOCX**: Paragraphs and tables extraction
- **TXT**: Multi-encoding support
- **CSV**: Automatic delimiter detection
- **XLSX/XLS**: Multiple sheets support

### Processing Pipeline
```python
File Upload ‚Üí Validation ‚Üí Content Extraction ‚Üí Metadata Creation ‚Üí Trigger Check
     ‚Üì             ‚Üì              ‚Üì                   ‚Üì               ‚Üì
Size/Type      Store File    Extract Text      Store Metadata   Workflow Trigger
```

## üîÑ Workflow Automation Features

### Step Types Available
1. **Log Step**: Custom logging with template variables
2. **File Process Step**: Extract content from uploaded files
3. **Airtable Create Step**: Create records in Airtable tables
4. **Airtable Update Step**: Update existing records
5. **Delay Step**: Add time delays between steps
6. **Condition Step**: Conditional execution logic

### Trigger Types
- **File Upload**: Triggered when specific file types are uploaded
- **File Processed**: Triggered after file processing completes
- **Scheduled**: Cron-based scheduling for recurring workflows

### Template Variables
- `{workflow_id}`, `{execution_id}` - Workflow identifiers
- `{filename}`, `{file_id}`, `{file_size}` - File-related data
- `{content}` - Extracted file content
- `{created_at}` - Timestamp information
- `{step_N_result}` - Results from previous steps

## üöÄ Immediate Priorities

1. **Enhanced Error Handling** (HIGH)
   ```python
   # Improve error recovery for failed workflow steps
   # Add retry mechanisms for transient failures
   # Better error reporting and logging
   ```

2. **Performance Optimization** (HIGH)
   ```python
   # Optimize file processing for large files
   # Implement streaming for file uploads
   # Add file processing caching
   ```

3. **Comprehensive Testing** (MEDIUM)
   ```python
   # Unit tests for all file processing formats
   # Integration tests for workflow execution
   # Load tests for concurrent file processing
   ```

## üîÆ Future Enhancements

### Phase 1 (Next Sprint)
- [ ] Enhanced file validation and virus scanning
- [ ] Workflow step templates and reusable components
- [ ] Real-time workflow execution monitoring
- [ ] File processing progress tracking

### Phase 2 (Next Month)
- [ ] Advanced workflow conditions and logic
- [ ] External service integrations (email, webhooks)
- [ ] Bulk file processing capabilities
- [ ] Workflow versioning and rollback

### Phase 3 (Future)
- [ ] Visual workflow builder interface
- [ ] Machine learning-powered content analysis
- [ ] Advanced file format support
- [ ] Distributed file processing

## ‚ö†Ô∏è Known Issues
1. **File size limits** - Currently 100MB max, could be increased
2. **Limited workflow debugging** - Need better execution visibility
3. **No workflow pause/resume** - Workflows run to completion or fail
4. **Basic file security** - Could add virus scanning and content filtering

## üß™ Testing Strategy
```python
# Priority test coverage:
- File upload and processing for all supported formats
- Workflow creation and execution end-to-end
- Trigger mechanism testing (file upload, scheduling)
- Error handling and recovery scenarios
- Performance testing for large files and complex workflows
```

## üìä Performance Targets
- **File Upload**: < 5s for files up to 100MB
- **Content Extraction**: < 10s for standard documents
- **Workflow Execution**: < 2s per step (excluding delays)
- **Concurrent Files**: 50+ simultaneous processing
- **Memory Usage**: < 500MB per worker

## ü§ù Service Dependencies
```
Frontend ‚Üí API Gateway ‚Üí Automation Services ‚Üí Airtable Gateway
                              ‚Üì                      ‚Üì
                         PostgreSQL            Airtable API
                              ‚Üì
                            Redis
```

## üí° Development Tips
1. Use template variables for dynamic workflow content
2. Test file processing with various file sizes and formats
3. Monitor Redis for background task queues
4. Use cron expressions for precise scheduling

## üö® Critical Configuration
```python
# Required environment variables:
DATABASE_URL=postgresql://...          # Database connection
REDIS_URL=redis://redis:6379          # Background task queue
UPLOAD_DIRECTORY=./uploads            # File storage location
MAX_FILE_SIZE=104857600              # 100MB file size limit
AIRTABLE_API_KEY=your_key            # For Airtable integrations
```

## üîí Security Considerations
- **File Validation**: Type and size limits enforced
- **Secure Storage**: Files stored with restricted access
- **Content Sanitization**: Extracted content is sanitized
- **Workflow Isolation**: Each execution runs in isolation
- **API Authentication**: JWT and API key support

## üìà Monitoring Metrics
```python
# Key metrics to track:
automation_files_uploaded_total{type}          # File upload counts
automation_files_processed_duration_seconds    # Processing time
automation_workflows_executed_total{status}    # Workflow execution
automation_workflow_step_duration_seconds      # Step performance
automation_file_storage_bytes_total           # Storage usage
```

## üéØ Consolidation Benefits

### Before vs After
- **Before**: File Processor (port 8005) + Workflow Engine (port 8007)
- **After**: Unified Automation Services (port 8006)

### Integration Benefits
- Cross-service triggers: Files automatically trigger workflows
- Shared background processing with unified task management
- Combined health checks and monitoring
- Reduced inter-service communication overhead

### Resource Efficiency
- 50% reduction in container overhead
- Shared database connections and Redis client
- Unified configuration and logging
- Single deployment and monitoring point

## üîÑ Service Communication
```python
# File processing flow:
Client ‚Üí API Gateway ‚Üí Automation Services ‚Üí File Storage
                             ‚Üì
                        PostgreSQL (metadata)

# Workflow execution flow:
Scheduler ‚Üí Workflow Engine ‚Üí Step Execution ‚Üí Airtable Gateway
                    ‚Üì               ‚Üì               ‚Üì
               PostgreSQL      Redis Queue    External APIs
```

Remember: This service is the **automation backbone** of PyAirtable - enabling intelligent file processing and complex workflow orchestration that bridges file content with Airtable operations!